{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Cybercrime Intelligence Newspaper Agent\n",
                "\n",
                "This notebook implements a simple autonomous AI agent that monitors news sources for cybercrime-related incidents. The agent automatically fetches newspaper articles, identifies fraud and cybercrime mentions, summarizes key information, and generates a daily intelligence brief for law enforcement agencies. This is a **Stage-1 prototype** focused on clarity and simplicity‚Äîdesigned to evolve into a production-ready system."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What is an AI Agent?\n",
                "\n",
                "Think of an AI agent as a **smart assistant that can act on its own**. Unlike a simple chatbot that just answers questions, an agent:\n",
                "\n",
                "1. **Perceives** its environment (reads news, checks data)\n",
                "2. **Thinks** about what it sees (analyzes, filters, decides)\n",
                "3. **Acts** on its analysis (summarizes, reports, alerts)\n",
                "4. **Repeats** this cycle autonomously\n",
                "\n",
                "In Ed Donner's framework, an agent has:\n",
                "- **Goals** (find cybercrime news)\n",
                "- **Tools** (web scraping, LLM reasoning)\n",
                "- **Memory** (what it has already processed)\n",
                "- **Actions** (generate reports)\n",
                "\n",
                "Our agent is simple but powerful: it replaces hours of manual news monitoring with an automated, intelligent system."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from datetime import datetime\n",
                "import json\n",
                "import re\n",
                "\n",
                "# For LLM integration - using Ollama with local Llama 3.2 model\n",
                "try:\n",
                "    import ollama\n",
                "    # Test if Ollama is running and model is available\n",
                "    try:\n",
                "        ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': 'test'}])\n",
                "        LLM_AVAILABLE = True\n",
                "        print(\"‚úÖ Ollama connected successfully with llama3.2:latest model\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Ollama error: {e}\")\n",
                "        print(\"   Make sure Ollama is running and llama3.2:latest model is downloaded\")\n",
                "        print(\"   Run: ollama pull llama3.2:latest\")\n",
                "        LLM_AVAILABLE = False\n",
                "except ImportError:\n",
                "    print(\"‚ö†Ô∏è Ollama library not installed. Install with: pip install ollama\")\n",
                "    print(\"   Will use mock summaries for demo.\")\n",
                "    LLM_AVAILABLE = False\n",
                "\n",
                "print(\"\\n‚úÖ Libraries imported successfully\")\n",
                "print(f\"üìÖ Today's date: {datetime.now().strftime('%Y-%m-%d')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Agent Loop Explained\n",
                "\n",
                "Our agent follows a simple four-step cycle:\n",
                "\n",
                "### 1Ô∏è‚É£ **FETCH** (Perception)\n",
                "- Retrieve news headlines from RSS feeds from major Indian news sources\n",
                "- Parse the content into structured data\n",
                "\n",
                "### 2Ô∏è‚É£ **THINK** (Reasoning)\n",
                "- Filter articles using cybercrime keywords\n",
                "- Identify which news items are relevant to law enforcement\n",
                "\n",
                "### 3Ô∏è‚É£ **ACT** (Decision Making)\n",
                "- Use an LLM to summarize relevant articles\n",
                "- Extract key insights and implications\n",
                "\n",
                "### 4Ô∏è‚É£ **REPORT** (Output)\n",
                "- Generate a clean, actionable intelligence brief\n",
                "- Present findings in a structured format\n",
                "\n",
                "This is the **core pattern** of agentic AI‚Äîsimple but infinitely scalable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 1: FETCH - Newspaper Article Fetcher\n",
                "\n",
                "def fetch_from_rss(rss_url, source_name):\n",
                "    \"\"\"\n",
                "    Fetch articles from an RSS feed.\n",
                "    \n",
                "    Args:\n",
                "        rss_url: URL of the RSS feed\n",
                "        source_name: Name of the news source\n",
                "    \n",
                "    Returns:\n",
                "        List of article dictionaries\n",
                "    \"\"\"\n",
                "    try:\n",
                "        headers = {\n",
                "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
                "        }\n",
                "        response = requests.get(rss_url, timeout=15, headers=headers)\n",
                "        response.raise_for_status()\n",
                "        \n",
                "        soup = BeautifulSoup(response.content, 'xml')\n",
                "        items = soup.find_all('item')\n",
                "        \n",
                "        articles = []\n",
                "        for item in items[:20]:  # Limit to 20 articles per source\n",
                "            title = item.title.text.strip() if item.title else ''\n",
                "            description = item.description.text.strip() if item.description else ''\n",
                "            link = item.link.text.strip() if item.link else ''\n",
                "            \n",
                "            # Clean HTML tags from description if present\n",
                "            if description:\n",
                "                description = BeautifulSoup(description, 'html.parser').get_text()\n",
                "            \n",
                "            if title:  # Only add if we have at least a title\n",
                "                articles.append({\n",
                "                    'title': title,\n",
                "                    'summary': description if description else title,\n",
                "                    'source': source_name,\n",
                "                    'url': link\n",
                "                })\n",
                "        \n",
                "        print(f\"   ‚úì {source_name}: {len(articles)} articles\")\n",
                "        return articles\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"   ‚úó {source_name}: Error - {str(e)[:50]}\")\n",
                "        return []\n",
                "\n",
                "\n",
                "def fetch_news_articles():\n",
                "    \"\"\"\n",
                "    Fetch news headlines from multiple Indian news sources via RSS feeds.\n",
                "    \n",
                "    Returns: List of article dictionaries with title, summary, source, and url\n",
                "    \"\"\"\n",
                "    \n",
                "    # Major Indian news sources with RSS feeds\n",
                "    rss_feeds = [\n",
                "        # Times of India\n",
                "        ('https://timesofindia.indiatimes.com/rssfeedstopstories.cms', 'Times of India'),\n",
                "        \n",
                "        # The Hindu\n",
                "        ('https://www.thehindu.com/news/national/feeder/default.rss', 'The Hindu'),\n",
                "        \n",
                "        # NDTV\n",
                "        ('https://feeds.feedburner.com/ndtvnews-top-stories', 'NDTV'),\n",
                "        \n",
                "        # India Today\n",
                "        ('https://www.indiatoday.in/rss/1206514', 'India Today'),\n",
                "        \n",
                "        # Hindustan Times\n",
                "        ('https://www.hindustantimes.com/feeds/rss/india-news/rssfeed.xml', 'Hindustan Times'),\n",
                "        \n",
                "        # Indian Express\n",
                "        ('https://indianexpress.com/feed/', 'Indian Express'),\n",
                "    ]\n",
                "    \n",
                "    all_articles = []\n",
                "    \n",
                "    print(\"üì∞ Fetching articles from news sources...\")\n",
                "    \n",
                "    for rss_url, source_name in rss_feeds:\n",
                "        articles = fetch_from_rss(rss_url, source_name)\n",
                "        all_articles.extend(articles)\n",
                "    \n",
                "    print(f\"\\nüìä Total articles fetched: {len(all_articles)}\")\n",
                "    return all_articles\n",
                "\n",
                "\n",
                "# Execute the fetch\n",
                "all_articles = fetch_news_articles()\n",
                "print(f\"\\n‚úÖ Stage 1 (FETCH) complete: {len(all_articles)} articles retrieved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 2: THINK - Filter for Cybercrime Content\n",
                "\n",
                "def filter_cybercrime_articles(articles):\n",
                "    \"\"\"\n",
                "    Filter articles for cybercrime-related keywords.\n",
                "    \n",
                "    This is a simple keyword-based approach.\n",
                "    In Stage-2, we could use LLM-based classification for better accuracy.\n",
                "    \n",
                "    Args:\n",
                "        articles: List of article dictionaries\n",
                "    \n",
                "    Returns:\n",
                "        List of filtered articles related to cybercrime\n",
                "    \"\"\"\n",
                "    \n",
                "    # Cybercrime-related keywords (government intelligence focus)\n",
                "    keywords = [\n",
                "        'fraud', 'scam', 'cybercrime', 'cyber crime', 'digital arrest',\n",
                "        'deepfake', 'phishing', 'hacking', 'ransomware', 'data breach',\n",
                "        'online fraud', 'upi fraud', 'cryptocurrency scam', 'ponzi scheme',\n",
                "        'identity theft', 'cyber attack', 'malware', 'fake website',\n",
                "        'cyber security', 'cybersecurity', 'hacker', 'cyber threat',\n",
                "        'online scam', 'digital fraud', 'bank fraud', 'credit card fraud'\n",
                "    ]\n",
                "    \n",
                "    filtered = []\n",
                "    \n",
                "    for article in articles:\n",
                "        # Combine title and summary for keyword matching\n",
                "        text = (article['title'] + ' ' + article['summary']).lower()\n",
                "        \n",
                "        # Check if any keyword appears in the text\n",
                "        for keyword in keywords:\n",
                "            if keyword in text:\n",
                "                article['matched_keyword'] = keyword  # Track what triggered the match\n",
                "                filtered.append(article)\n",
                "                break  # One match is enough\n",
                "    \n",
                "    print(f\"üéØ Filtered to {len(filtered)} cybercrime-related articles\")\n",
                "    print(f\"   (Removed {len(articles) - len(filtered)} irrelevant articles)\")\n",
                "    \n",
                "    return filtered\n",
                "\n",
                "\n",
                "# Execute the filtering\n",
                "cybercrime_articles = filter_cybercrime_articles(all_articles)\n",
                "\n",
                "# Show what we found\n",
                "print(\"\\nüìã Cybercrime articles identified:\")\n",
                "for idx, article in enumerate(cybercrime_articles, 1):\n",
                "    print(f\"   {idx}. {article['title'][:60]}...\")\n",
                "    print(f\"      Matched keyword: '{article['matched_keyword']}'\")\n",
                "\n",
                "print(f\"\\n‚úÖ Stage 2 (THINK) complete: {len(cybercrime_articles)} relevant articles\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 3: ACT - LLM Summarization and Analysis\n",
                "\n",
                "def summarize_with_llm(article):\n",
                "    \"\"\"\n",
                "    Use an LLM to create intelligence-focused summaries.\n",
                "    \n",
                "    The LLM extracts:\n",
                "    - Key facts (who, what, where, when)\n",
                "    - Law enforcement implications\n",
                "    - Actionable insights\n",
                "    \n",
                "    Args:\n",
                "        article: Dictionary with article data\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with intelligence summary\n",
                "    \"\"\"\n",
                "    \n",
                "    if not LLM_AVAILABLE:\n",
                "        # Fallback: Return a basic summary without LLM\n",
                "        return {\n",
                "            'title': article['title'],\n",
                "            'summary': article['summary'][:200] + '...' if len(article['summary']) > 200 else article['summary'],\n",
                "            'implications': 'LLM not available - using basic summary',\n",
                "            'source': article['source'],\n",
                "            'url': article.get('url', 'N/A')\n",
                "        }\n",
                "    \n",
                "    # Create a focused prompt for intelligence analysis\n",
                "    prompt = f\"\"\"\n",
                "You are an intelligence analyst for a government cybercrime unit.\n",
                "\n",
                "Article Title: {article['title']}\n",
                "Article Content: {article['summary']}\n",
                "Source: {article['source']}\n",
                "\n",
                "Provide a brief intelligence summary (2-3 sentences) covering:\n",
                "1. Key facts (who, what, amounts involved)\n",
                "2. Why this matters for law enforcement\n",
                "3. Any patterns or trends\n",
                "\n",
                "Keep it concise and actionable.\n",
                "\"\"\"\n",
                "    \n",
                "    try:\n",
                "        # Use Ollama with local Llama 3.2 model\n",
                "        response = ollama.chat(\n",
                "            model='llama3.2:latest',\n",
                "            messages=[\n",
                "                {\n",
                "                    'role': 'system',\n",
                "                    'content': 'You are a cybercrime intelligence analyst. Provide concise, factual analysis.'\n",
                "                },\n",
                "                {\n",
                "                    'role': 'user',\n",
                "                    'content': prompt\n",
                "                }\n",
                "            ],\n",
                "            options={\n",
                "                'temperature': 0.3,  # Lower temperature for factual analysis\n",
                "                'num_predict': 200   # Limit response length\n",
                "            }\n",
                "        )\n",
                "        \n",
                "        intelligence_summary = response['message']['content'].strip()\n",
                "        \n",
                "        return {\n",
                "            'title': article['title'],\n",
                "            'summary': intelligence_summary,\n",
                "            'implications': 'Analyzed by Llama 3.2 (local)',\n",
                "            'source': article['source'],\n",
                "            'url': article.get('url', 'N/A')\n",
                "        }\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è LLM error for article '{article['title'][:30]}...': {str(e)[:50]}\")\n",
                "        return {\n",
                "            'title': article['title'],\n",
                "            'summary': article['summary'][:200] + '...' if len(article['summary']) > 200 else article['summary'],\n",
                "            'implications': 'LLM analysis failed - using original summary',\n",
                "            'source': article['source'],\n",
                "            'url': article.get('url', 'N/A')\n",
                "        }\n",
                "\n",
                "\n",
                "# Process all filtered articles\n",
                "intelligence_summaries = []\n",
                "\n",
                "print(\"ü§ñ Analyzing articles with Llama 3.2 (local model)...\\n\")\n",
                "\n",
                "for idx, article in enumerate(cybercrime_articles, 1):\n",
                "    print(f\"   Processing {idx}/{len(cybercrime_articles)}: {article['title'][:50]}...\")\n",
                "    summary = summarize_with_llm(article)\n",
                "    intelligence_summaries.append(summary)\n",
                "\n",
                "print(f\"\\n‚úÖ Stage 3 (ACT) complete: {len(intelligence_summaries)} summaries generated\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 4: REPORT - Generate Daily Intelligence Brief\n",
                "\n",
                "def generate_intelligence_report(summaries):\n",
                "    \"\"\"\n",
                "    Generate a clean, actionable daily intelligence brief.\n",
                "    \n",
                "    This is what gets delivered to law enforcement analysts.\n",
                "    \"\"\"\n",
                "    \n",
                "    report_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
                "    \n",
                "    # Build the report\n",
                "    report = f\"\"\"\n",
                "{'='*80}\n",
                "    DAILY CYBERCRIME INTELLIGENCE BRIEF\n",
                "    Generated: {report_date}\n",
                "    Source: Automated Newspaper Agent (Real-time RSS Feeds)\n",
                "    AI Model: Llama 3.2 (Local)\n",
                "{'='*80}\n",
                "\n",
                "üìä SUMMARY\n",
                "   ‚Ä¢ Total articles scanned: {len(all_articles)}\n",
                "   ‚Ä¢ Cybercrime incidents identified: {len(summaries)}\n",
                "   ‚Ä¢ Analysis method: {'Llama 3.2 Local LLM' if LLM_AVAILABLE else 'Keyword-based'}\n",
                "   ‚Ä¢ News sources: Times of India, The Hindu, NDTV, India Today, Hindustan Times, Indian Express\n",
                "\n",
                "{'='*80}\n",
                "\n",
                "üö® KEY INCIDENTS\n",
                "\n",
                "\"\"\"\n",
                "    \n",
                "    # Add each incident\n",
                "    for idx, summary in enumerate(summaries, 1):\n",
                "        report += f\"\"\"\n",
                "{'‚îÄ'*80}\n",
                "INCIDENT #{idx}\n",
                "{'‚îÄ'*80}\n",
                "\n",
                "üì∞ Headline:\n",
                "   {summary['title']}\n",
                "\n",
                "üîç Intelligence Summary:\n",
                "   {summary['summary']}\n",
                "\n",
                "üìå Source: {summary['source']}\n",
                "üîó URL: {summary.get('url', 'N/A')}\n",
                "\n",
                "\"\"\"\n",
                "    \n",
                "    # Add footer\n",
                "    report += f\"\"\"\n",
                "{'='*80}\n",
                "\n",
                "üìù NOTES\n",
                "   ‚Ä¢ This is an automated prototype using real-time RSS feeds and local Llama 3.2 model\n",
                "   ‚Ä¢ Human verification recommended for high-priority incidents\n",
                "   ‚Ä¢ For urgent matters, contact the Cybercrime Coordination Center\n",
                "   ‚Ä¢ Report generated from live news sources at {report_date}\n",
                "\n",
                "{'='*80}\n",
                "End of Report\n",
                "{'='*80}\n",
                "\"\"\"\n",
                "    \n",
                "    return report\n",
                "\n",
                "\n",
                "# Generate and display the final report\n",
                "final_report = generate_intelligence_report(intelligence_summaries)\n",
                "print(final_report)\n",
                "\n",
                "# Optionally save to file\n",
                "report_filename = f\"cybercrime_brief_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
                "with open(report_filename, 'w', encoding='utf-8') as f:\n",
                "    f.write(final_report)\n",
                "\n",
                "print(f\"\\nüíæ Report saved to: {report_filename}\")\n",
                "print(\"\\n‚úÖ Stage 4 (REPORT) complete: Intelligence brief generated\")\n",
                "print(\"\\nüéâ AGENT CYCLE COMPLETE!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Future Enhancements (Stage-2 and Beyond)\n",
                "\n",
                "This Stage-1 prototype demonstrates the core agent pattern with real-time data. Here's how we can evolve it:\n",
                "\n",
                "### **Immediate Next Steps (Stage-2)**\n",
                "- **Automation**: Schedule the agent to run every 6 hours using cron jobs or cloud functions\n",
                "- **More RSS Feeds**: Add regional news sources and specialized cybercrime publications\n",
                "- **Better Classification**: Replace keyword matching with LLM-based relevance scoring\n",
                "- **Sentiment Analysis**: Track public sentiment around cybercrime incidents\n",
                "- **Deduplication**: Identify and merge duplicate stories from different sources\n",
                "\n",
                "### **Medium-Term (Stage-3)**\n",
                "- **Admin Dashboard**: Build a web interface for viewing reports and managing alerts\n",
                "- **Alert System**: Send notifications for high-priority incidents (email, SMS, Slack)\n",
                "- **Trend Detection**: Track recurring patterns and emerging threats over time\n",
                "- **Multi-Source Integration**: Add social media monitoring (Twitter, Reddit)\n",
                "- **Database Storage**: Store articles and summaries for historical analysis\n",
                "\n",
                "### **Advanced (Stage-4 - Multi-Agent System)**\n",
                "- **Specialist Agents**: Deploy separate agents for different cybercrime types\n",
                "  - Agent 1: Financial fraud (UPI, cryptocurrency)\n",
                "  - Agent 2: Social engineering (phishing, deepfakes)\n",
                "  - Agent 3: Infrastructure attacks (ransomware, DDoS)\n",
                "- **Coordinator Agent**: Synthesizes findings from all specialist agents\n",
                "- **Vector Database**: Store and search historical intelligence reports\n",
                "- **Predictive Analytics**: Use ML to forecast cybercrime trends\n",
                "\n",
                "### **Production Deployment**\n",
                "- **API Integration**: Connect with government databases and case management systems\n",
                "- **Compliance**: Ensure data privacy and security standards (encryption, access logs)\n",
                "- **Human-in-the-Loop**: Allow analysts to provide feedback and refine agent behavior\n",
                "- **Scalability**: Deploy on cloud infrastructure (AWS, Azure, GCP) for 24/7 operation\n",
                "\n",
                "---\n",
                "\n",
                "### **Key Principle (Ed Donner Style)**\n",
                "\n",
                "> *\"Start simple, prove value, then scale.\"*\n",
                "\n",
                "This notebook now uses real data and proves the concept works with live news sources. Now we can confidently add complexity where it matters most.\n",
                "\n",
                "---\n",
                "\n",
                "**Questions? Feedback?**  \n",
                "This is a living prototype. Test it, break it, improve it. That's how great agents are built. üõ†Ô∏è"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}