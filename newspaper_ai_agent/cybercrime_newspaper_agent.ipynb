{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Chhattisgarh Police - Crime Intelligence Analysis Agent\n",
                "\n",
                "This notebook implements an AI agent for **Chhattisgarh Police Department** that analyzes newspaper clipping screenshots to extract crime-related information. The agent:\n",
                "\n",
                "1. **Accepts** a newspaper clipping screenshot from the user\n",
                "2. **Extracts** text using OCR (Optical Character Recognition)\n",
                "3. **Searches** RSS feeds with **priority on Chhattisgarh regional news**\n",
                "4. **Aggregates** information from various sources\n",
                "5. **Generates** a comprehensive unbiased summary\n",
                "6. **Identifies** common terms across all news sources\n",
                "7. **Provides** investigative clues to help Chhattisgarh Police with their investigation\n",
                "\n",
                "This is an **interactive analysis tool** designed specifically for **Chhattisgarh Police Department** to quickly gather intelligence from newspaper clippings, with emphasis on regional crime news."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìö Setup: Import Libraries\n",
                "\n",
                "We need several libraries for this agent:\n",
                "- **pytesseract**: OCR for text extraction from images\n",
                "- **PIL (Pillow)**: Image processing\n",
                "- **requests & BeautifulSoup**: Web scraping for RSS feeds\n",
                "- **ollama**: Local LLM for intelligent analysis\n",
                "- **collections**: For common term extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ PIL and pytesseract imported successfully\n"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from datetime import datetime\n",
                "import json\n",
                "import re\n",
                "from collections import Counter\n",
                "import os\n",
                "\n",
                "# Image processing and OCR\n",
                "try:\n",
                "    from PIL import Image\n",
                "    import pytesseract\n",
                "    OCR_AVAILABLE = True\n",
                "    print(\"‚úÖ PIL and pytesseract imported successfully\")\n",
                "    # Note: You may need to set the tesseract path on Windows\n",
                "    # pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
                "except ImportError as e:\n",
                "    print(f\"‚ö†Ô∏è OCR libraries not available: {e}\")\n",
                "    print(\"   Install with: pip install pytesseract pillow\")\n",
                "    print(\"   Also install Tesseract-OCR: https://github.com/tesseract-ocr/tesseract\")\n",
                "    OCR_AVAILABLE = False\n",
                "\n",
                "# For LLM integration - using Ollama with local Llama 3.2 model\n",
                "try:\n",
                "    import ollama\n",
                "    # Test if Ollama is running and model is available\n",
                "    try:\n",
                "        ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': 'test'}])\n",
                "        LLM_AVAILABLE = True\n",
                "        print(\"‚úÖ Ollama connected successfully with llama3.2:latest model\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Ollama error: {e}\")\n",
                "        print(\"   Make sure Ollama is running and llama3.2:latest model is downloaded\")\n",
                "        print(\"   Run: ollama pull llama3.2:latest\")\n",
                "        LLM_AVAILABLE = False\n",
                "except ImportError:\n",
                "    print(\"‚ö†Ô∏è Ollama library not installed. Install with: pip install ollama\")\n",
                "    print(\"   Will use basic summaries for demo.\")\n",
                "    LLM_AVAILABLE = False\n",
                "\n",
                "print(\"\\n‚úÖ Libraries imported successfully\")\n",
                "print(f\"üìÖ Today's date: {datetime.now().strftime('%Y-%m-%d')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì∏ Step 1: Upload Newspaper Clipping & Extract Text\n",
                "\n",
                "Provide the path to your newspaper clipping screenshot. The agent will use OCR to extract the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üì∏ Extracting text from image...\n",
                        "\n",
                        "================================================================================\n",
                        "EXTRACTED TEXT FROM NEWSPAPER CLIPPING\n",
                        "================================================================================\n",
                        "Mob loots police armoury in\n",
                        "Manipur; policeman killed\n",
                        "\n",
                        "The Hindu Bureau\n",
                        "NEW DELHI\n",
                        "\n",
                        "A police armoury was loot-\n",
                        "ed and a Manipur Rifles\n",
                        "policeman was killed in\n",
                        "Manipur on Thursday, a\n",
                        "police official said. Anoth-\n",
                        "er attempt to loot weapons\n",
                        "in Imphal city was thwart-\n",
                        "ed by the police.\n",
                        "\n",
                        "The incidents come a\n",
                        "day ahead of a Supreme\n",
                        "Court hearing on the mat-\n",
                        "ter.\n",
                        "\n",
                        "The first incident took\n",
                        "place in Bishnupur district\n",
                        "when a mob looted auto-\n",
                        "matic weapons from the\n",
                        "second Indian Reserve Bat-\n",
                        "talion (IRB) at Naransena.\n",
                        "The number of weapons\n",
                        "looted from the armoury is\n",
                        "not known.\n",
                        "\n",
                        "Curfew relaxations\n",
                        "in Imphal East and\n",
                        "Imphal West were\n",
                        "withdrawn and\n",
                        "restrictions Imposed\n",
                        "\n",
                        "Many people had gath-\n",
                        "ered around 12 km away to\n",
                        "protest a tribal group‚Äôs call\n",
                        "for mass burial of 35 Kuki-\n",
                        "Zo people who were killed\n",
                        "in the ethnic violence that\n",
                        "erupted in the State on\n",
                        "May 3.\n",
                        "\n",
                        "The protesters led by\n",
                        "women tried to storm the\n",
                        "way to the area where the\n",
                        "burial was planned but\n",
                        "were stopped by the Rapid\n",
                        "Action Force (RAF) and the\n",
                        "Army.\n",
                        "\n",
                        "Though the burial had\n",
                        "\n",
                        "been called off after the\n",
                        "Union Home Ministry in-\n",
                        "tervened and spoke to the\n",
                        "tribal leaders, the protes-\n",
                        "ters wanted to proceed to\n",
                        "the area. The said field is a\n",
                        "government notified land.\n",
                        "\n",
                        "The second incident\n",
                        "took place at Kourtuk on\n",
                        "Imphal West-Kangpokpi\n",
                        "border where a policeman\n",
                        "attached to 6 Manipur Ri-\n",
                        "fles, identified as T. Rishi\n",
                        "(48), was shot in the head\n",
                        "by armed miscreants from\n",
                        "the hill areas.\n",
                        "\n",
                        "Meanwhile, curfew re-\n",
                        "laxations in Imphal East\n",
                        "and Imphal West were\n",
                        "withdrawn and restrictions\n",
                        "imposed during the day as\n",
                        "a precautionary measure.\n",
                        "Relaxation has been given\n",
                        "only from 5 a.m. to noon.\n",
                        "================================================================================\n",
                        "\n",
                        "‚úÖ Text extraction complete\n"
                    ]
                }
            ],
            "source": [
                "def extract_text_from_image(image_path):\n",
                "    \"\"\"\n",
                "    Extract text from a newspaper clipping image using OCR.\n",
                "    \n",
                "    Args:\n",
                "        image_path: Path to the image file\n",
                "    \n",
                "    Returns:\n",
                "        Extracted text as string\n",
                "    \"\"\"\n",
                "    if not OCR_AVAILABLE:\n",
                "        return \"OCR not available. Please install pytesseract and Tesseract-OCR.\"\n",
                "    \n",
                "    try:\n",
                "        # Open and process the image\n",
                "        img = Image.open(image_path)\n",
                "        \n",
                "        # Extract text using pytesseract\n",
                "        text = pytesseract.image_to_string(img)\n",
                "        \n",
                "        # Clean up the text\n",
                "        text = text.strip()\n",
                "        \n",
                "        if not text:\n",
                "            return \"No text could be extracted from the image. Please check the image quality.\"\n",
                "        \n",
                "        return text\n",
                "    \n",
                "    except FileNotFoundError:\n",
                "        return f\"Error: Image file not found at {image_path}\"\n",
                "    except Exception as e:\n",
                "        return f\"Error extracting text: {str(e)}\"\n",
                "\n",
                "\n",
                "# USER INPUT: Provide the path to your newspaper clipping image\n",
                "# Example: image_path = r\"C:\\Users\\HP\\Desktop\\newspaper_clipping.jpg\"\n",
                "image_path = input(\"Enter the path to your newspaper clipping image: \").strip('\"').strip(\"'\")\n",
                "\n",
                "print(\"\\nüì∏ Extracting text from image...\")\n",
                "extracted_text = extract_text_from_image(image_path)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXTRACTED TEXT FROM NEWSPAPER CLIPPING\")\n",
                "print(\"=\"*80)\n",
                "print(extracted_text)\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n‚úÖ Text extraction complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîç Step 2: Extract Keywords for Search\n",
                "\n",
                "Analyze the extracted text to identify key search terms and entities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Extracting keywords for search...\n",
                        "\n",
                        "üìã Found 11 search keywords:\n",
                        "   1. police\n",
                        "   2. killed\n",
                        "   3. weapon\n",
                        "   4. Manipur\n",
                        "   5. Hindu\n",
                        "   6. Bureau\n",
                        "   7. DELHI\n",
                        "   8. Rifles\n",
                        "   9. Thursday\n",
                        "   10. Anoth-\n",
                        "   11. Imphal\n",
                        "\n",
                        "‚úÖ Keyword extraction complete\n"
                    ]
                }
            ],
            "source": [
                "def extract_keywords(text):\n",
                "    \"\"\"\n",
                "    Extract important keywords and entities from the text for searching.\n",
                "    \n",
                "    Args:\n",
                "        text: Extracted text from newspaper clipping\n",
                "    \n",
                "    Returns:\n",
                "        List of keywords\n",
                "    \"\"\"\n",
                "    # Crime-related keywords to prioritize\n",
                "    crime_keywords = [\n",
                "        'fraud', 'scam', 'cybercrime', 'theft', 'robbery', 'murder', 'assault',\n",
                "        'kidnapping', 'arrest', 'police', 'investigation', 'suspect', 'victim',\n",
                "        'crime', 'criminal', 'gang', 'shooting', 'killed', 'attack', 'hacking',\n",
                "        'phishing', 'ransomware', 'deepfake', 'smuggling', 'trafficking', 'drug',\n",
                "        'corruption', 'bribery', 'extortion', 'weapon', 'bomb', 'terrorist'\n",
                "    ]\n",
                "    \n",
                "    # Convert to lowercase for matching\n",
                "    text_lower = text.lower()\n",
                "    \n",
                "    # Find crime keywords present in the text\n",
                "    found_keywords = [kw for kw in crime_keywords if kw in text_lower]\n",
                "    \n",
                "    # Extract potential names (capitalized words)\n",
                "    words = text.split()\n",
                "    capitalized = [w.strip('.,!?;:\"()[]') for w in words if w and w[0].isupper() and len(w) > 3]\n",
                "    \n",
                "    # Extract numbers (amounts, dates, etc.)\n",
                "    numbers = re.findall(r'\\d+', text)\n",
                "    \n",
                "    # Combine all keywords\n",
                "    all_keywords = found_keywords + capitalized[:10]  # Limit capitalized words\n",
                "    \n",
                "    # Remove duplicates while preserving order\n",
                "    seen = set()\n",
                "    unique_keywords = []\n",
                "    for kw in all_keywords:\n",
                "        if kw.lower() not in seen:\n",
                "            seen.add(kw.lower())\n",
                "            unique_keywords.append(kw)\n",
                "    \n",
                "    return unique_keywords[:15]  # Return top 15 keywords\n",
                "\n",
                "\n",
                "# Extract keywords from the newspaper clipping\n",
                "print(\"üîç Extracting keywords for search...\")\n",
                "search_keywords = extract_keywords(extracted_text)\n",
                "\n",
                "print(f\"\\nüìã Found {len(search_keywords)} search keywords:\")\n",
                "for idx, keyword in enumerate(search_keywords, 1):\n",
                "    print(f\"   {idx}. {keyword}\")\n",
                "\n",
                "print(\"\\n‚úÖ Keyword extraction complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì∞ Step 3: Search RSS Feeds for Related Articles\n",
                "\n",
                "Search multiple news sources for articles related to the newspaper clipping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Searching for related articles...\n",
                        "\n",
                        "üì∞ Fetching articles from news sources...\n",
                        "   ‚úì Times of India: 30 articles fetched\n",
                        "   ‚úì The Hindu: 30 articles fetched\n",
                        "   ‚úì NDTV: 20 articles fetched\n",
                        "   ‚úì India Today: 20 articles fetched\n",
                        "   ‚úì Hindustan Times: 30 articles fetched\n",
                        "   ‚úì Indian Express: 30 articles fetched\n",
                        "\n",
                        "üìä Total articles fetched: 160\n",
                        "\n",
                        "‚úÖ Found 21 related articles\n",
                        "\n",
                        "üìã Top 10 related articles:\n",
                        "\n",
                        "   1. Indian-origin techie from Karnataka shot dead in ‚Äòtargeted‚Äô attack outside Canad...\n",
                        "      Source: Times of India\n",
                        "      Matched keywords: police, killed\n",
                        "      Relevance score: 2\n",
                        "\n",
                        "   2. Delhi pit death: Cops probe if contractors tried ‚Äòcovering up lapses‚Äô...\n",
                        "      Source: Times of India\n",
                        "      Matched keywords: police, DELHI\n",
                        "      Relevance score: 2\n",
                        "\n",
                        "   3. No copies of Naravane‚Äôs memoir have gone into publication: Publisher...\n",
                        "      Source: The Hindu\n",
                        "      Matched keywords: police, DELHI\n",
                        "      Relevance score: 2\n",
                        "\n",
                        "   4. Delhi biker who fell in Janakpuri pit likely died of asphyxiation, autopsy sugge...\n",
                        "      Source: Hindustan Times\n",
                        "      Matched keywords: police, DELHI\n",
                        "      Relevance score: 2\n",
                        "\n",
                        "   5. ‚ÄòIf I had died like Nirbhaya, everyone'd believe me‚Äô: Unnao survivor seeks death...\n",
                        "      Source: Times of India\n",
                        "      Matched keywords: DELHI\n",
                        "      Relevance score: 1\n",
                        "\n",
                        "   6. First digital trail emerges in Ghaziabad sisters' suicide case, Phone recovered ...\n",
                        "      Source: Times of India\n",
                        "      Matched keywords: police\n",
                        "      Relevance score: 1\n",
                        "\n",
                        "   7. Haryana: Faridabad DCP (NIT) transferred after Surajkund mela tragedy...\n",
                        "      Source: The Hindu\n",
                        "      Matched keywords: police\n",
                        "      Relevance score: 1\n",
                        "\n",
                        "   8. Journalist arrested for objectionable Facebook post on U.P. CM Adityanath in Bal...\n",
                        "      Source: The Hindu\n",
                        "      Matched keywords: police\n",
                        "      Relevance score: 1\n",
                        "\n",
                        "   9. Delhi High Court seeks CBI stand on Jaideep Sengar‚Äôs plea for extension of inter...\n",
                        "      Source: The Hindu\n",
                        "      Matched keywords: DELHI\n",
                        "      Relevance score: 1\n",
                        "\n",
                        "   10. Hyderabad and Cyberabad traffic police book over 900 drunk driving cases in spec...\n",
                        "      Source: The Hindu\n",
                        "      Matched keywords: police\n",
                        "      Relevance score: 1\n"
                    ]
                }
            ],
            "source": [
                "def fetch_from_rss(rss_url, source_name):\n",
                "    \"\"\"\n",
                "    Fetch articles from an RSS feed.\n",
                "    \n",
                "    Args:\n",
                "        rss_url: URL of the RSS feed\n",
                "        source_name: Name of the news source\n",
                "    \n",
                "    Returns:\n",
                "        List of article dictionaries\n",
                "    \"\"\"\n",
                "    try:\n",
                "        headers = {\n",
                "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
                "        }\n",
                "        response = requests.get(rss_url, timeout=15, headers=headers)\n",
                "        response.raise_for_status()\n",
                "        \n",
                "        soup = BeautifulSoup(response.content, 'xml')\n",
                "        items = soup.find_all('item')\n",
                "        \n",
                "        articles = []\n",
                "        for item in items[:30]:  # Get more articles for better matching\n",
                "            title = item.title.text.strip() if item.title else ''\n",
                "            description = item.description.text.strip() if item.description else ''\n",
                "            link = item.link.text.strip() if item.link else ''\n",
                "            pub_date = item.pubDate.text.strip() if item.pubDate else ''\n",
                "            \n",
                "            # Clean HTML tags from description if present\n",
                "            if description:\n",
                "                description = BeautifulSoup(description, 'html.parser').get_text()\n",
                "            \n",
                "            if title:  # Only add if we have at least a title\n",
                "                articles.append({\n",
                "                    'title': title,\n",
                "                    'summary': description if description else title,\n",
                "                    'source': source_name,\n",
                "                    'url': link,\n",
                "                    'pub_date': pub_date\n",
                "                })\n",
                "        \n",
                "        return articles\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"   ‚úó {source_name}: Error - {str(e)[:50]}\")\n",
                "        return []\n",
                "\n",
                "\n",
                "def search_related_articles(keywords):\n",
                "    \"\"\"\n",
                "    Search RSS feeds for articles matching the keywords.\n",
                "    \n",
                "    Args:\n",
                "        keywords: List of search keywords\n",
                "    \n",
                "    Returns:\n",
                "        List of related articles\n",
                "    \"\"\"\n",
                "    # Major Indian news sources with RSS feeds\n",
                "    rss_feeds = [\n",
                "        ('https://timesofindia.indiatimes.com/rssfeedstopstories.cms', 'Times of India'),\n",
                "        ('https://www.thehindu.com/news/national/feeder/default.rss', 'The Hindu'),\n",
                "        ('https://feeds.feedburner.com/ndtvnews-top-stories', 'NDTV'),\n",
                "        ('https://www.indiatoday.in/rss/1206514', 'India Today'),\n",
                "        ('https://www.hindustantimes.com/feeds/rss/india-news/rssfeed.xml', 'Hindustan Times'),\n",
                "        ('https://indianexpress.com/feed/', 'Indian Express'),\n",
                "    ]\n",
                "    \n",
                "    all_articles = []\n",
                "    \n",
                "    print(\"üì∞ Fetching articles from news sources...\")\n",
                "    \n",
                "    for rss_url, source_name in rss_feeds:\n",
                "        articles = fetch_from_rss(rss_url, source_name)\n",
                "        all_articles.extend(articles)\n",
                "        print(f\"   ‚úì {source_name}: {len(articles)} articles fetched\")\n",
                "    \n",
                "    print(f\"\\nüìä Total articles fetched: {len(all_articles)}\")\n",
                "    \n",
                "    # Filter articles that match any of the keywords\n",
                "    related_articles = []\n",
                "    \n",
                "    for article in all_articles:\n",
                "        text = (article['title'] + ' ' + article['summary']).lower()\n",
                "        \n",
                "        # Check if any keyword appears in the article\n",
                "        matched_keywords = []\n",
                "        for keyword in keywords:\n",
                "            if keyword.lower() in text:\n",
                "                matched_keywords.append(keyword)\n",
                "        \n",
                "        if matched_keywords:\n",
                "            article['matched_keywords'] = matched_keywords\n",
                "            article['relevance_score'] = len(matched_keywords)\n",
                "            related_articles.append(article)\n",
                "    \n",
                "    # Sort by relevance (number of matched keywords)\n",
                "    related_articles.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
                "    \n",
                "    return related_articles\n",
                "\n",
                "\n",
                "# Search for related articles\n",
                "print(\"\\nüîç Searching for related articles...\\n\")\n",
                "related_articles = search_related_articles(search_keywords)\n",
                "\n",
                "print(f\"\\n‚úÖ Found {len(related_articles)} related articles\")\n",
                "\n",
                "# Display top matches\n",
                "if related_articles:\n",
                "    print(\"\\nüìã Top 10 related articles:\")\n",
                "    for idx, article in enumerate(related_articles[:10], 1):\n",
                "        print(f\"\\n   {idx}. {article['title'][:80]}...\")\n",
                "        print(f\"      Source: {article['source']}\")\n",
                "        print(f\"      Matched keywords: {', '.join(article['matched_keywords'][:5])}\")\n",
                "        print(f\"      Relevance score: {article['relevance_score']}\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è No related articles found. Try adjusting the search keywords.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîó Step 4: Extract Common Terms Across Sources\n",
                "\n",
                "Identify frequently mentioned terms across all related articles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîó Extracting common terms across all sources...\n",
                        "\n",
                        "üìä Common terms mentioned across news sources:\n",
                        "\n",
                        "============================================================\n",
                        "TERM                      FREQUENCY       SOURCES\n",
                        "============================================================\n",
                        "police                    15              5 sources\n",
                        "delhi                     15              4 sources\n",
                        "court                     5               2 sources\n",
                        "gold                      5               1 sources\n",
                        "year                      4               3 sources\n",
                        "kumar                     4               2 sources\n",
                        "death                     4               1 sources\n",
                        "over                      4               4 sources\n",
                        "died                      4               3 sources\n",
                        "case                      4               3 sources\n",
                        "book                      4               2 sources\n",
                        "killed                    3               2 sources\n",
                        "years                     3               2 sources\n",
                        "digital                   3               2 sources\n",
                        "survivor                  3               1 sources\n",
                        "sengar                    3               2 sources\n",
                        "bail                      3               2 sources\n",
                        "faridabad                 3               1 sources\n",
                        "february                  3               2 sources\n",
                        "today                     3               1 sources\n",
                        "government                3               2 sources\n",
                        "chief                     3               2 sources\n",
                        "indian                    2               1 sources\n",
                        "karnataka                 2               1 sources\n",
                        "targeted                  2               1 sources\n",
                        "============================================================\n",
                        "\n",
                        "‚úÖ Common terms extraction complete\n"
                    ]
                }
            ],
            "source": [
                "def extract_common_terms(articles, top_n=20):\n",
                "    \"\"\"\n",
                "    Extract common terms mentioned across multiple articles.\n",
                "    \n",
                "    Args:\n",
                "        articles: List of article dictionaries\n",
                "        top_n: Number of top terms to return\n",
                "    \n",
                "    Returns:\n",
                "        List of (term, count) tuples\n",
                "    \"\"\"\n",
                "    # Stop words to filter out\n",
                "    stop_words = {\n",
                "        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
                "        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
                "        'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
                "        'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these',\n",
                "        'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which',\n",
                "        'who', 'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both',\n",
                "        'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
                "        'only', 'own', 'same', 'so', 'than', 'too', 'very', 'said', 'after',\n",
                "        'also', 'into', 'through', 'during', 'before', 'after', 'above',\n",
                "        'below', 'between', 'under', 'again', 'further', 'then', 'once'\n",
                "    }\n",
                "    \n",
                "    # Collect all words from articles\n",
                "    all_words = []\n",
                "    \n",
                "    for article in articles:\n",
                "        text = article['title'] + ' ' + article['summary']\n",
                "        # Extract words (alphanumeric, length > 3)\n",
                "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text.lower())\n",
                "        all_words.extend(words)\n",
                "    \n",
                "    # Filter out stop words\n",
                "    filtered_words = [w for w in all_words if w not in stop_words]\n",
                "    \n",
                "    # Count occurrences\n",
                "    word_counts = Counter(filtered_words)\n",
                "    \n",
                "    # Return top N terms\n",
                "    return word_counts.most_common(top_n)\n",
                "\n",
                "\n",
                "# Extract common terms\n",
                "if related_articles:\n",
                "    print(\"\\nüîó Extracting common terms across all sources...\")\n",
                "    common_terms = extract_common_terms(related_articles, top_n=25)\n",
                "    \n",
                "    print(\"\\nüìä Common terms mentioned across news sources:\")\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"{'TERM':<25} {'FREQUENCY':<15} {'SOURCES'}\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    for term, count in common_terms:\n",
                "        # Count how many different sources mention this term\n",
                "        sources = set()\n",
                "        for article in related_articles:\n",
                "            text = (article['title'] + ' ' + article['summary']).lower()\n",
                "            if term in text:\n",
                "                sources.add(article['source'])\n",
                "        \n",
                "        print(f\"{term:<25} {count:<15} {len(sources)} sources\")\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "    print(\"\\n‚úÖ Common terms extraction complete\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è No articles to analyze for common terms\")\n",
                "    common_terms = []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ñ Step 5: Generate Comprehensive Analysis with LLM\n",
                "\n",
                "Use the LLM to create an unbiased summary and extract investigative clues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'related_articles' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    101\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mfull_analysis\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    102\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    103\u001b[39m         }\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Generate comprehensive analysis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrelated_articles\u001b[49m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mü§ñ Generating comprehensive analysis and investigative clues...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m     analysis_result = analyze_for_investigation(extracted_text, related_articles, common_terms)\n",
                        "\u001b[31mNameError\u001b[39m: name 'related_articles' is not defined"
                    ]
                }
            ],
            "source": [
                "def analyze_for_investigation(original_text, articles, common_terms):\n",
                "    \"\"\"\n",
                "    Use LLM to generate comprehensive analysis and investigative clues.\n",
                "    \n",
                "    Args:\n",
                "        original_text: Text extracted from newspaper clipping\n",
                "        articles: Related articles from multiple sources\n",
                "        common_terms: Common terms across sources\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with analysis results\n",
                "    \"\"\"\n",
                "    if not LLM_AVAILABLE:\n",
                "        return {\n",
                "            'full_analysis': 'LLM not available for analysis',\n",
                "            'status': 'error'\n",
                "        }\n",
                "    \n",
                "    # Prepare article summaries for context (limit to top 5 for faster processing)\n",
                "    articles_context = \"\\n\\n\".join([\n",
                "        f\"Source: {a['source']}\\nTitle: {a['title']}\\nContent: {a['summary'][:200]}...\"\n",
                "        for a in articles[:5]  # Reduced from 10 to 5 for faster processing\n",
                "    ])\n",
                "    \n",
                "    # Prepare common terms context\n",
                "    terms_context = \", \".join([term for term, _ in common_terms[:10]])\n",
                "    \n",
                "    # Create comprehensive prompt\n",
                "    prompt = f\"\"\"\n",
                "You are a senior crime intelligence analyst assisting police investigation.\n",
                "\n",
                "ORIGINAL NEWSPAPER CLIPPING TEXT:\n",
                "{original_text[:500]}\n",
                "\n",
                "RELATED ARTICLES FROM MULTIPLE NEWS SOURCES:\n",
                "{articles_context}\n",
                "\n",
                "COMMON TERMS ACROSS ALL SOURCES:\n",
                "{terms_context}\n",
                "\n",
                "Please provide a comprehensive analysis with the following sections:\n",
                "\n",
                "1. UNBIASED SUMMARY: Combine information from all sources into a factual, unbiased summary (3-4 sentences). Avoid speculation.\n",
                "\n",
                "2. KEY ENTITIES: List all important entities mentioned:\n",
                "   - Suspects/Accused\n",
                "   - Victims\n",
                "   - Locations\n",
                "   - Organizations\n",
                "   - Amounts/Items involved\n",
                "\n",
                "3. TIMELINE: Reconstruct the sequence of events based on available information.\n",
                "\n",
                "4. INVESTIGATIVE CLUES: Identify specific clues that could help police investigation:\n",
                "   - Potential leads to follow\n",
                "   - Patterns or connections\n",
                "   - Digital evidence mentioned\n",
                "   - Witnesses or informants\n",
                "   - Modus operandi\n",
                "\n",
                "5. INVESTIGATION RECOMMENDATIONS: Suggest specific actions for police:\n",
                "   - Priority investigation areas\n",
                "   - Evidence to collect\n",
                "   - Experts to consult\n",
                "   - Cross-referencing with other cases\n",
                "\n",
                "Be specific, factual, and actionable. Focus on information that helps law enforcement.\n",
                "\"\"\"\n",
                "    \n",
                "    try:\n",
                "        print(\"   ü§ñ Analyzing with Llama 3.2 (this may take 30-60 seconds)...\")\n",
                "        \n",
                "        response = ollama.chat(\n",
                "            model='llama3.2:latest',\n",
                "            messages=[\n",
                "                {\n",
                "                    'role': 'system',\n",
                "                    'content': 'You are a senior crime intelligence analyst. Provide factual, unbiased analysis to assist police investigations.'\n",
                "                },\n",
                "                {\n",
                "                    'role': 'user',\n",
                "                    'content': prompt\n",
                "                }\n",
                "            ],\n",
                "            options={\n",
                "                'temperature': 0.3,  # Lower temperature for factual analysis\n",
                "                'num_predict': 800   # Reduced from 1000 to 800 for faster response\n",
                "            }\n",
                "        )\n",
                "        \n",
                "        analysis = response['message']['content'].strip()\n",
                "        \n",
                "        return {\n",
                "            'full_analysis': analysis,\n",
                "            'status': 'success'\n",
                "        }\n",
                "    \n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ö†Ô∏è LLM analysis error: {str(e)[:100]}\")\n",
                "        return {\n",
                "            'full_analysis': f\"Error during analysis: {str(e)}\",\n",
                "            'status': 'error'\n",
                "        }\n",
                "\n",
                "\n",
                "# Generate comprehensive analysis\n",
                "if related_articles:\n",
                "    print(\"\\nü§ñ Generating comprehensive analysis and investigative clues...\\n\")\n",
                "    analysis_result = analyze_for_investigation(extracted_text, related_articles, common_terms)\n",
                "    \n",
                "    print(\"\\n‚úÖ Analysis complete\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è No articles available for analysis\")\n",
                "    analysis_result = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã Step 6: Generate Final Investigation Report\n",
                "\n",
                "Compile all findings into a comprehensive report for law enforcement."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_investigation_report(original_text, articles, common_terms, analysis):\n",
                "    \"\"\"\n",
                "    Generate a comprehensive investigation report.\n",
                "    \n",
                "    Args:\n",
                "        original_text: Text from newspaper clipping\n",
                "        articles: Related articles\n",
                "        common_terms: Common terms across sources\n",
                "        analysis: LLM analysis result\n",
                "    \n",
                "    Returns:\n",
                "        Formatted report string\n",
                "    \"\"\"\n",
                "    report_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
                "    \n",
                "    report = f\"\"\"\n",
                "{'='*80}\n",
                "    CRIME INTELLIGENCE INVESTIGATION REPORT\n",
                "    Generated: {report_date}\n",
                "    Analysis Type: Multi-Source Newspaper Analysis\n",
                "    AI Model: Llama 3.2 (Local)\n",
                "{'='*80}\n",
                "\n",
                "SECTION 1: ORIGINAL NEWSPAPER CLIPPING TEXT\n",
                "{'‚îÄ'*80}\n",
                "{original_text}\n",
                "{'‚îÄ'*80}\n",
                "\n",
                "SECTION 2: RELATED ARTICLES FOUND\n",
                "{'‚îÄ'*80}\n",
                "Total articles found: {len(articles)}\n",
                "News sources: {len(set(a['source'] for a in articles))} different sources\n",
                "\n",
                "Top 10 Most Relevant Articles:\n",
                "\n",
                "\"\"\"\n",
                "    \n",
                "    # Add top articles\n",
                "    for idx, article in enumerate(articles[:10], 1):\n",
                "        report += f\"\"\"\n",
                "{idx}. {article['title']}\n",
                "   Source: {article['source']}\n",
                "   URL: {article.get('url', 'N/A')}\n",
                "   Relevance Score: {article.get('relevance_score', 0)}\n",
                "   Matched Keywords: {', '.join(article.get('matched_keywords', [])[:5])}\n",
                "\n",
                "\"\"\"\n",
                "    \n",
                "    # Add common terms\n",
                "    report += f\"\"\"\n",
                "{'‚îÄ'*80}\n",
                "\n",
                "SECTION 3: COMMON TERMS ACROSS ALL SOURCES\n",
                "{'‚îÄ'*80}\n",
                "These terms appear frequently across multiple news sources:\n",
                "\n",
                "\"\"\"\n",
                "    \n",
                "    for term, count in common_terms[:15]:\n",
                "        # Count sources\n",
                "        sources = set()\n",
                "        for article in articles:\n",
                "            text = (article['title'] + ' ' + article['summary']).lower()\n",
                "            if term in text:\n",
                "                sources.add(article['source'])\n",
                "        \n",
                "        report += f\"  ‚Ä¢ {term.upper()}: mentioned {count} times across {len(sources)} sources\\n\"\n",
                "    \n",
                "    # Add LLM analysis\n",
                "    report += f\"\"\"\n",
                "\n",
                "{'‚îÄ'*80}\n",
                "\n",
                "SECTION 4: COMPREHENSIVE ANALYSIS & INVESTIGATIVE CLUES\n",
                "{'‚îÄ'*80}\n",
                "\"\"\"\n",
                "    \n",
                "    if analysis and analysis.get('status') == 'success':\n",
                "        report += analysis['full_analysis']\n",
                "    else:\n",
                "        report += \"LLM analysis not available.\\n\"\n",
                "    \n",
                "    # Add footer\n",
                "    report += f\"\"\"\n",
                "\n",
                "{'='*80}\n",
                "\n",
                "REPORT NOTES:\n",
                "  ‚Ä¢ This report combines information from {len(articles)} articles across {len(set(a['source'] for a in articles))} news sources\n",
                "  ‚Ä¢ Analysis performed using local Llama 3.2 model for unbiased intelligence\n",
                "  ‚Ä¢ All information should be verified through official investigation channels\n",
                "  ‚Ä¢ For urgent matters, contact the Crime Coordination Center\n",
                "  ‚Ä¢ Report generated at: {report_date}\n",
                "\n",
                "{'='*80}\n",
                "End of Investigation Report\n",
                "{'='*80}\n",
                "\"\"\"\n",
                "    \n",
                "    return report\n",
                "\n",
                "\n",
                "# Generate and display final report\n",
                "if related_articles and analysis_result:\n",
                "    print(\"\\nüìã Generating final investigation report...\\n\")\n",
                "    \n",
                "    final_report = generate_investigation_report(\n",
                "        extracted_text,\n",
                "        related_articles,\n",
                "        common_terms,\n",
                "        analysis_result\n",
                "    )\n",
                "    \n",
                "    print(final_report)\n",
                "    \n",
                "    # Save to file\n",
                "    report_filename = f\"investigation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
                "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
                "        f.write(final_report)\n",
                "    \n",
                "    print(f\"\\nüíæ Report saved to: {report_filename}\")\n",
                "    print(\"\\n‚úÖ Investigation report generation complete\")\n",
                "    print(\"\\nüéâ ANALYSIS COMPLETE!\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Cannot generate report - insufficient data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Summary\n",
                "\n",
                "This notebook successfully:\n",
                "\n",
                "1. ‚úÖ Extracted text from newspaper clipping using OCR\n",
                "2. ‚úÖ Identified key search terms and entities\n",
                "3. ‚úÖ Searched multiple RSS feeds for related articles\n",
                "4. ‚úÖ Found common terms across all news sources\n",
                "5. ‚úÖ Generated comprehensive unbiased summary\n",
                "6. ‚úÖ Provided investigative clues for police\n",
                "7. ‚úÖ Created detailed investigation report\n",
                "\n",
                "### Next Steps for Law Enforcement:\n",
                "\n",
                "- Review the investigative clues section carefully\n",
                "- Cross-reference with existing case databases\n",
                "- Follow up on leads identified in the analysis\n",
                "- Verify all information through official channels\n",
                "- Use common terms to identify patterns across cases\n",
                "\n",
                "---\n",
                "\n",
                "**Note**: This is an AI-assisted analysis tool. All findings should be verified through proper investigation procedures."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
